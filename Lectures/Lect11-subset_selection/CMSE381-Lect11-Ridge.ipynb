{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfd68f5",
   "metadata": {},
   "source": [
    "# Lec 11: Ridge Regression\n",
    "## CMSE 381 - Spring 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90aa0b0",
   "metadata": {},
   "source": [
    "In this module we are going to test out the ridge regression method we discussed in class from Chapter 6.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everyone's favorite standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9a257",
   "metadata": {},
   "source": [
    "# Loading in the data\n",
    "\n",
    "Ok, here we go, let's play with a baseball data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_df = pd.read_csv('../../DataSets/Hitters.csv')\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3549cda",
   "metadata": {},
   "source": [
    "Annoyingly enough we have some missing values in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9032bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of null values:\", hitters_df[\"Salary\"].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40baef",
   "metadata": {},
   "source": [
    "So let's go clean those up....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dimensions of the original Hitters data (322 rows x 20 columns)\n",
    "print(\"Dimensions of original data:\", hitters_df.shape)\n",
    "\n",
    "# Drop any rows the contain missing values, along with the player names\n",
    "hitters_df = hitters_df.dropna().drop('Player', axis=1)\n",
    "\n",
    "# Print the dimensions of the modified Hitters data (263 rows x 20 columns)\n",
    "print(\"Dimensions of modified data:\", hitters_df.shape)\n",
    "\n",
    "# One last check: should return 0\n",
    "print(\"Number of null values:\", hitters_df[\"Salary\"].isnull().sum())\n",
    "\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a060d0",
   "metadata": {},
   "source": [
    "And finally, we can replace our categorical variables with dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_df = pd.get_dummies(hitters_df, drop_first = True)\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hitters_df.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary)\n",
    "X = hitters_df.drop(['Salary'], axis = 1).astype('float64')\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3656818c",
   "metadata": {},
   "source": [
    "## Normalizing the data \n",
    "\n",
    "Our first job is to normalize the data. Right now, all our columns have different standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7660b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d801e4a",
   "metadata": {},
   "source": [
    "We'll do it on all the columns, but I'll draw pictures just with the `Hits` column for ease of visualization. Here's that the distribution of that particular column is before doing any normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea209d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(X.Hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3daa2",
   "metadata": {},
   "source": [
    "I'm going to take all my columns and normalize them so that each column has standard deviation 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa689698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):    \n",
    "  return (X-X.mean())/X.std()\n",
    "X_norm = normalize(X)\n",
    "X_norm.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd91f2",
   "metadata": {},
   "source": [
    "Woohoo, I have normalized data! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1a144",
   "metadata": {},
   "source": [
    "Notice that the shape of the `Hits` histogram is the same, we've just scaled the $x$-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad309ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(X_norm.Hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f83fd5",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "In class, we learned that doing ridge regression means that we try to find the best model accoding to the score\n",
    "$$\n",
    "RSS + \\lambda \\sum_{i} \\beta_i^2.\n",
    "$$\n",
    "The good news is that `scikitlearn` has a built in `Ridge` function.  \n",
    "\n",
    "- [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "- [User guide](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21cc7f7",
   "metadata": {},
   "source": [
    "The bad (ok, not honestly that bad) news is that they call their $\\lambda$ parameter $\\alpha$. So we're just going to minimize \n",
    "$$\n",
    "RSS + \\alpha \\sum_{i} \\beta_i^2.\n",
    "$$\n",
    "instead. So if I pick an $\\alpha$ value, I can do ridge regression as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e548cf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = 1 #<------ this is me picking an alpha value\n",
    "X_norm = normalize(X)\n",
    "# Fit the regression\n",
    "ridge = Ridge(alpha = a) \n",
    "ridge.fit(X_norm, y)\n",
    "\n",
    "# Get all the coefficients\n",
    "print('intercept:', ridge.intercept_)\n",
    "print('\\n')\n",
    "print(pd.Series(ridge.coef_, index = X_norm.columns))\n",
    "print('\\nTraining MSE:',mean_squared_error(y,ridge.predict(X_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d25cce",
   "metadata": {},
   "source": [
    "Of course, that was just me picking a random $\\alpha$ out of a hat so there's no reason to trust that it's a good one. I could sit here all day and move that $\\alpha$ around to see what's going on, but why do that, when I can make a for loop!\n",
    "\n",
    "Here's a pile of $\\alpha$s for us to test on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a875cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(4,-2,100)*0.5\n",
    "alphas = np.append(alphas,0)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7d033",
   "metadata": {},
   "source": [
    "First off, let's take a look at how the coefficients learned change for various choices of $\\alpha$. \n",
    "\n",
    "Associated with each alpha value is a vector of ridge regression coefficients, which we'll store in a matrix coefs. In this case, it is a  19Ã—100  matrix, with 19 rows (one for each predictor) and 100 columns (one for each value of alpha). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    model =  Ridge(alpha = a)\n",
    "    model.fit(X_norm, y)\n",
    "    coefs.append(model.coef_)\n",
    "    \n",
    "\n",
    "coefs = pd.DataFrame(coefs,columns = X_norm.columns)\n",
    "coefs.head()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9f5b3",
   "metadata": {},
   "source": [
    "Let's say I just want to look at the coefficient for the `Hits` column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d66723",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas,coefs.Hits, label = 'Hits')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97909990",
   "metadata": {},
   "source": [
    "Or just for the `Runs` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e516a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas,coefs.Runs, label = 'Runs')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad8f06",
   "metadata": {},
   "source": [
    "But that's pretty annoying, so instead we look at all of the coefficients at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ae261",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for var in coefs.columns:\n",
    "    plt.plot(alphas, coefs[var], label = var)\n",
    "plt.xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('coefficients')\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1.1,1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for plotting errors \n",
    "# also added grey color for ease of visualization\n",
    "for var in coefs.columns:\n",
    "    if np.abs(coefs[var][100])<200:\n",
    "        plt.plot(alphas, coefs[var], label = var, color = 'grey', linewidth = .5)\n",
    "    else:\n",
    "        plt.plot(alphas, coefs[var], label = var)\n",
    "    \n",
    "plt.xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('coefficients')\n",
    "\n",
    "plt.legend(bbox_to_anchor = (1.1,1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1269c032",
   "metadata": {},
   "source": [
    "## Train/test split version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3012f155",
   "metadata": {},
   "source": [
    "Now we can start setting up the usual train/test splits to have at least a starting idea of how the testing error is going. The `random_state=1` bit just makes it so that everyone should get the same random split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d835cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee551fba",
   "metadata": {},
   "source": [
    "\n",
    "&#9989; **<font color=red>Do this:</font>** Train a model using ridge regression with $\\alpha = 4$. What is the MSE of your model on the testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f78b6",
   "metadata": {},
   "source": [
    "\n",
    "&#9989; **<font color=red>Do this:</font>** Ha ha nah, you can do better than that.  Lets try all our alphas and take a look at the testing MSE to make a better decision about what $\\alpha$ we might want. Modify the code below to plot your testing MSE for all the alphas. What $\\alpha$ should we use to train the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify your code from above and add it in the for loop to plot the testing MSE\n",
    "\n",
    "ridge = Ridge()\n",
    "errors = []\n",
    "\n",
    "for a in alphas:\n",
    "    # ==== Your code goes in here ==== #\n",
    "    errors.append(17) #<----- random number in here so that the code runs before you fix it\n",
    "\n",
    "#---plotting----\n",
    "\n",
    "plt.plot(alphas,errors)\n",
    "plt.title('Testing MSE')\n",
    "ax=plt.gca()\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e0478",
   "metadata": {},
   "source": [
    "## RidgeCV\n",
    "\n",
    "Whelp, your meanie professor didn't tell you that there's actually a built in function to do this for you (sorry-not-sorry). Aren't you glad you didn't read ahead?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d1510b",
   "metadata": {},
   "source": [
    "- [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)\n",
    "- [User Guide](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)\n",
    "\n",
    "Basically, `RidgeCV` runs LOOCV (unless you tell it otherwise, see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)) on all the $\\alpha$ values you specify on an input array, and tells you the best $\\alpha$ given that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c751b",
   "metadata": {},
   "source": [
    "First, I am going to normalize. I'm being careful to not just normalize all of $X$ and then pass this matrix on since this is an example of [data leakage](https://scikit-learn.org/stable/common_pitfalls.html#data-leakage-during-pre-processing). In that version, we are using some amount of information about the testing data (in this case it is contributing to the normalization process) to affect something in our training process. So here's what we do instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(X_norm, y, test_size=0.3, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b437b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to drop that 0 from the alphas because it makes \n",
    "# RidgeCV cranky\n",
    "alphas = alphas[:-1]\n",
    "\n",
    "\n",
    "ridgecv = RidgeCV(alphas = alphas, \n",
    "                  scoring = 'neg_mean_squared_error', \n",
    "                  )\n",
    "ridgecv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print('alpha chosen is', ridgecv.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7759291",
   "metadata": {},
   "source": [
    "I can predict my values on the test set directly from the `ridgecv` model we just built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8577eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ridgecv.predict(X_test)\n",
    "mean_squared_error(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe0dfc5",
   "metadata": {},
   "source": [
    "This is exactly the same result as if I went and retrained my model using the chosen $\\alpha$ using `Ridge`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha = ridgecv.alpha_)\n",
    "ridge.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, ridge.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5384c",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Why did we get a different best choice of $\\alpha$ than we found in the previous section? \n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8354f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
