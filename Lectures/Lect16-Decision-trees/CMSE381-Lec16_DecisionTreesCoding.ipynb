{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfd68f5",
   "metadata": {},
   "source": [
    "# Lecture 16: Decision Trees\n",
    "## CMSE 381 - Spring 2024\n",
    "\n",
    "![](https://img.icons8.com/emoji/344/deciduous-tree-emoji.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90aa0b0",
   "metadata": {},
   "source": [
    "In this module we are going to test out the tree based methods we discussed in class from Chapter 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea3a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everyone's favorite standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "\n",
    "# ML imports we've used previously\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707e03e",
   "metadata": {},
   "source": [
    "# Fitting Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e564ac",
   "metadata": {},
   "source": [
    "We can now turn to setting up a basic regression tree. For this example, we're going to use the `Carseat` data where we will predict `Sales` from the rest of the columns. I'll do a bit of cleanup for you so we can get to the good stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29199e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats = pd.read_csv('../../DataSets/Carseats.csv').drop('Unnamed: 0', axis=1)\n",
    "carseats.ShelveLoc = pd.factorize(carseats.ShelveLoc)[0]\n",
    "carseats.Urban = carseats.Urban.map({'No':0, 'Yes':1})\n",
    "carseats.US = carseats.US.map({'No':0, 'Yes':1})\n",
    "carseats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633522b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122a990",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = carseats.drop(['Sales'], axis = 1)\n",
    "y = carseats.Sales\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063d040",
   "metadata": {},
   "source": [
    "The regression tree function we will use is `DecisionTreeRegressor`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8347c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3612a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_tree = DecisionTreeRegressor(max_depth = 3)\n",
    "reg_tree.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6367aa6",
   "metadata": {},
   "source": [
    "We can draw the resulting tree to visualize what's happening. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d308fb",
   "metadata": {},
   "source": [
    "## Visualization 1: Text based\n",
    "\n",
    "Ok, so not the prettiest of the options, but at least this one will work. This provides a text based tree where I can figure out what decisions were made at each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f0ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( tree.export_text(reg_tree, feature_names = list(X.columns)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb618ca",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>**  Given a new data point with the entries below, use the visualization to determine the choices made by the decision tree at each step? What will your decision tree predict for `Sales`?\n",
    "\n",
    "```\n",
    "CompPrice      117\n",
    "Income         100\n",
    "Advertising      4\n",
    "Population     466\n",
    "Price           97\n",
    "ShelveLoc        2\n",
    "Age             55\n",
    "Education       14\n",
    "Urban            1\n",
    "US               1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416c983",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3fd5e4",
   "metadata": {},
   "source": [
    "## Visualization 2: Probably should work \n",
    "\n",
    "Here's another option for visualization. There is a plotting function built into `sklearn.tree` but I've had issues with people's python versions before.  Let's try it and see, it's a bit clunky but it gets the job done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19851a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (25,20))\n",
    "_= tree.plot_tree(reg_tree, feature_names = X.columns, \n",
    "               filled = True, \n",
    "              fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee5fbc",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Given a new data point with the following entries, use the visualization to determine the choices made by the decision tree at each step? What will your decision tree predict for `Sales`?\n",
    "\n",
    "```\n",
    "CompPrice      141\n",
    "Income          64\n",
    "Advertising      3\n",
    "Population     340\n",
    "Price          128\n",
    "ShelveLoc        0\n",
    "Age             38\n",
    "Education       13\n",
    "Urban            1\n",
    "US               0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac679250",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46145c6d",
   "metadata": {},
   "source": [
    "## Other visualization tools. \n",
    "\n",
    "There are nicer visualization tools. In particular, the outputs requiring `graphviz` are quite a bit better than these options.  However, installing `graphviz` is non trivial so we won't use it in this lecture. Examples of code using this can be found [here](https://mljar.com/blog/visualize-decision-tree/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf62e5",
   "metadata": {},
   "source": [
    "## Predicting on the tree. \n",
    "\n",
    "As with all other `sklearn` packages we've seen, we can predict values on our input `X` matrix, and compare the results using MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = reg_tree.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152a66f",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Use the regression tree you just built to predict the `Sales` value for the test set. \n",
    "- Check your answers from above.  The first data point example was the third row of  `X`; the second data point example was the fourth row. Do you get the same answer from the prediction as by hand with the visualization?\n",
    "- What is the resulting MSE on the full data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e1d1c",
   "metadata": {},
   "source": [
    "# Classification trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273acc26",
   "metadata": {},
   "source": [
    "## Loading in the data \n",
    "\n",
    "Let's start with the `palmerPenguins` data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import palmerpenguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_df = palmerpenguins.load_penguins()\n",
    "\n",
    "#I'm shuffling the data to make this a bit more interesting\n",
    "penguins_df = penguins_df.sample(frac=1, random_state=1236) \n",
    "\n",
    "penguins_df = penguins_df.dropna()\n",
    "penguins_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.get_dummies(penguins_df.drop(columns = ['species']), drop_first = True)\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9cdf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = penguins_df.species\n",
    "y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c886ccf5",
   "metadata": {},
   "source": [
    "# Fitting Classification Trees \n",
    "\n",
    "We'll use `sklearn`'s built in modules for this. As always, the [user guide](https://scikit-learn.org/stable/modules/tree.html) is an excellent place to get started. \n",
    "\n",
    "Now to fit the decision tree classifier? All we need is two lines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01949e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3cea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree = tree.DecisionTreeClassifier(max_depth = 3)\n",
    "clf_tree = clf_tree.fit(X_df, y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db7059",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Using the `.predict` function, what is the species predicted for the first five data points in `X`? Which of these predicted values are the same as the original labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b748a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79e3f2",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Use whichever visualization method you prefer from above to see the resulting tree. What is the sequence of decisions for predicting the first data point? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5139b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9f558",
   "metadata": {},
   "source": [
    "## Pruning the tree\n",
    "\n",
    "The simplest method we have for pruning the tree is to limit the maximum depth, that is the number of times the tree is allowed to split. \n",
    "\n",
    "&#9989; **<font color=red>Do this:</font>** Change the value of `max_depth` below to see how the resulting tree changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2169708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree = tree.DecisionTreeClassifier(max_depth = 10) #<-- mess with this\n",
    "\n",
    "\n",
    "clf_tree = clf_tree.fit(X_df, y_df)\n",
    "fig = plt.figure(figsize = (25,20))\n",
    "_= tree.plot_tree(clf_tree, feature_names = X_df.columns, \n",
    "               filled = True, \n",
    "              fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621d4a0",
   "metadata": {},
   "source": [
    "If you are interested in more complex pruning techniques like we discussed in class, you can try to mess around with [Minimal Cost-Complexity Pruning](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html), but I'll leave that for another day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a02dbed",
   "metadata": {},
   "source": [
    "## Visualizing the parameter splits \n",
    "Now, if we want to visualize the parameter splits that are being represented with the trees, we can do that.  However, I can't (easily) draw these sorts of figures when I'm using more than two variables. Let's just grab the first two variables and build a classifer off of those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb02bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pair = X_df[['bill_length_mm', 'bill_depth_mm']].values\n",
    "\n",
    "clf_pair = tree.DecisionTreeClassifier(max_depth = 5).fit(X_pair, y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f1f3c",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Use whatever worked for you above to plot your decision tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc75b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad94b8",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Below is some code that will draw the regions of parameter space that get each different prediction.  \n",
    "- Which labels do the colors red, yellow, and blue match to? \n",
    "- What split in the figure does the first split in your tree above correspond to? \n",
    "- What changes in this figure if you change the `max_depth` in your tree model above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f295148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds for the figure \n",
    "x0_min = X[:,0].min()-1\n",
    "x0_max = X[:,0].max()+1\n",
    "x1_min = X[:,1].min()-1\n",
    "x1_max = X[:,1].max()+1\n",
    "\n",
    "# Parameters\n",
    "n_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "plot_step = 0.02\n",
    "\n",
    "xx, yy = np.meshgrid(\n",
    "    np.arange(x0_min, x0_max, plot_step), np.arange(x1_min, x1_max, plot_step)\n",
    ")\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "Z = clf_pair.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "def numReplace(label):\n",
    "    if label == 'Adelie':\n",
    "        return 0\n",
    "    if label == 'Gentoo':\n",
    "        return 1\n",
    "    else: #(if 'Chinstrap')\n",
    "        return 2\n",
    "Z = np.array([numReplace(label) for label in Z])\n",
    "# Z = Z.replace(['Adelie','Gentoo','Chinstrap'], [0,1,2])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "plt.xlabel(X_df.columns[0])\n",
    "plt.ylabel(X_df.columns[1])\n",
    "\n",
    "# Plot the training points\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(\n",
    "        X[idx, 0],\n",
    "        X[idx, 1],\n",
    "        c=color,\n",
    "        label=['Adelie','Gentoo','Chinstrap'][i],\n",
    "        edgecolor=\"black\",\n",
    "        s=15,\n",
    "    )\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8354f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
